Overview of the AI Insights System:
The AI Insights system is designed to provide intelligent analysis of data based on user questions. It combines several technologies:
OpenAI's GPT model for natural language processing and generation
Pinecone for vector database storage and similarity search
RAG (Retrieval-Augmented Generation) technique for enhancing AI responses with relevant data
Data Processing and Storage:
a. Initial Data Handling:
The system starts with a large dataset (parsedData in your Redux store).
This data is truncated to a manageable size (CHUNK_SIZE MAX_CHUNKS) to avoid overwhelming the system.
b. Data Chunking:
The truncated data is divided into smaller chunks (CHUNK_SIZE of 100 rows each).
This chunking allows for more efficient processing and storage.
c. Embedding Generation:
For each chunk, the system generates an embedding using OpenAI's text-embedding-3-small model.
An embedding is a high-dimensional vector representation of the text data, capturing semantic meaning.
d. Pinecone Storage:
Each chunk's embedding is stored in Pinecone, a vector database optimized for similarity search.
Along with the embedding, metadata about the chunk (including a summary of the chunk data) is stored.
RAG (Retrieval-Augmented Generation) Process:
a. User Query Embedding:
When a user asks a question, the system generates an embedding for this question using the same embedding model.
b. Relevant Data Retrieval:
The question embedding is used to query Pinecone for the most similar data chunks.
Pinecone performs a similarity search, returning the top 5 most relevant chunks.
c. Context Preparation:
The system retrieves the metadata (including chunk summaries) for these relevant chunks.
It also prepares a summary of the entire dataset and other contextual information.
d. AI Query Generation:
All this information (question, data summary, relevant chunks) is formatted into a prompt for the AI model.
e. AI Response Generation:
The prompt is sent to OpenAI's GPT-4 model (gpt-4o-2024-08-06).
The AI generates a comprehensive response, including an executive summary, key metrics, main insights, action items, and limitations.
4. Response Processing and Presentation:
a. JSON Parsing:
The AI's response is expected to be in JSON format.
The system attempts to parse this JSON, with fallback mechanisms for handling parsing errors.
b. Data Structuring:
The parsed insights are structured into an InsightResponse object.
c. UI Rendering:
The insights are displayed in the UI using various components like accordions and cards.
The information is organized into sections: Executive Summary, Key Metrics, Main Insights, Action Items, and Limitations.
5. Key Components and Their Roles:
a. src/pages/api/ai-insights.ts:
Handles API requests for embedding generation, chunk storage, and insight generation.
Interfaces with OpenAI and Pinecone services.
Processes and structures the AI's response.
b. src/services/aiService.ts:
Manages the flow of data processing and AI interaction.
Handles data truncation, chunking, and the RAG process.
Provides functions for generating insights and custom analyses.
c. src/components/AIInsights/AIInsights.tsx:
Renders the UI for user interaction and insight display.
Manages state for user input, loading, and received insights.
Handles error states and progress indication.
6. Technical Details:
a. Embedding Model: text-embedding-3-small
b. Language Model: gpt-4o-2024-08-06
c. Vector Database: Pinecone
d. Max Tokens for AI Response: 16000
e. Chunk Size: 100 rows
f. Max Chunks Processed: 50
Error Handling and Limitations:
The system includes robust error handling for API failures, JSON parsing errors, and data processing issues.
It has limitations on the amount of data it can process (5000 rows maximum) to manage computational resources and API limits.
Security and Environment Variables:
API keys for OpenAI and Pinecone are stored securely in environment variables.
The system uses these keys to authenticate requests to external services.
This comprehensive system allows for intelligent, context-aware responses to user queries about large datasets, combining the power of vector databases, embeddings, and large language models in a RAG architecture.